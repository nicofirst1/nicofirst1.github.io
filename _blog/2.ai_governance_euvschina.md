---
title: "Exploring Global AI Governance: Lessons from China for the EU"
slug: china-eu-ai
image: /assets/images/news/global_governance.jpg
date: 2024-09-18
description: " Explore contrasting approaches to AI regulation. Can the EU adapt China's transparency measures without compromising free expression?"
---

```DISCALIMER:** The views and opinions expressed in this blog are solely my own and do not reflect those of my employer, or any of its affiliates. ```

##### **1. Introduction**
Recently, I’ve been reading a lot about how different countries are handling the regulation of AI, focusing particularly on China and the European Union (EU). What I found interesting is that, while both are dealing with the same core issues, they’re doing it in completely different ways. While the EU aims to create a **pro-active, horizontal**[^1] legal framework to regulate AI across sectors, China takes a more **reactive, vertical**[^1] approach—focused heavily on **controlling the flow of information.**

[^1]: 1) Read [this](https://www.holisticai.com/blog/regulating-ai-the-horizontal-vs-vertical-approach) for an in-depth explanation of vertical vs horizontal policing.


What caught my attention is how central information control is to [China’s strategy](https://carnegieendowment.org/research/2023/07/chinas-ai-regulations-and-how-they-get-made?lang=en), particularly when it comes to **recommendation algorithms**. Their priority is to ensure that content aligns with **political and social stability**. Of course, this is not something that could be easily replicated in the EU, given the complexity of its political landscape. But there are still lessons to be learned, especially when it comes to **fighting misinformation**.

In this post, I'll explore how China's regulation of AI recommendation systems effectively limits the spread of false information. I'll discuss how the EU might adapt some of these practical policy measures—like enhancing transparency, enforcing algorithmic accountability, and assessing AI systems' societal impact, without compromising its dedication to free speech and democratic principles. I'll also tie these ideas back to my earlier reflections on how AI can exacerbate the loneliness epidemic, emphasizing the need for companies to take responsibility for the societal consequences of their AI systems.


##### **2. The EU AI Act: A Broad, Horizontal Approach**
  
The [EU AI Act](https://www.europarl.europa.eu/doceo/document/TA-9-2024-0138_EN.pdf) aims to establish a comprehensive set of rules applicable to AI systems across different sectors. It follows a [risk-based approach](https://www.responsible.ai/the-eu-ai-act-explained-tracking-developments-for-responsible-ai/), categorizing AI systems based on the **level of risk** they pose to safety, privacy, or fundamental rights.  Higher-risk applications face stricter regulations, while lower-risk systems enjoy more flexibility.

The aim is to have a unified framework that ensures AI is regulated across the board. With the risk-based model, the Act is designed to focus more attention on high-risk AI applications, making sure they are handled with extra caution, while allowing low-risk systems to have more flexibility

One of the main reasons behind this approach is to avoid confusion. With so many countries in the EU, each with its own regulations, having a single set of rules helps ensure consistency. This way, AI developers and companies can follow the same guidelines across borders without worrying about conflicting laws or different standards. It’s a smart way to keep things in sync across the EU and prevent any fragmentation that could hold back innovation.


##### **3. China’s Approach: Prioritizing Information Control**
China's AI regulation centers on **controlling the flow of information** to maintain societal and political stability. A key focus is on **recommendation algorithms**, which are closely regulated to ensure they align with **government guidelines** and do not disseminate content that could disrupt **social harmony.**

Unlike the EU’s broad, risk-based framework, China takes a much more vertical and reactive stance. Rather than creating one overarching set of rules for all AI systems, they focus on specific applications that have the most immediate impact. In particular, recommendation systems and content management are closely regulated because of their role in shaping public discourse. China’s government requires platforms to carefully adjust their algorithms, ensuring that the content promoted aligns with political guidelines and doesn’t contribute to instability. 

Another example of China’s regulation is their approach to automatic pricing algorithms. In sectors like food, housing, and medicine, these algorithms are closely monitored to make sure they aren’t taking unfair advantage of users. By regulating how prices are set, China ensures that essentials remain affordable for the average citizen, adding another layer of social stability.

One of the more unique aspects of China’s governance strategy is the algorithm registry which serves as a central repository for algorithms that have the potential to shape public opinion or mobilize citizens. Developers are required to submit detailed reports on how these algorithms are trained and deployed, including the datasets they rely on. This mandatory filing gives the Chinese government a clearer view of how these systems operate and allows for closer monitoring to ensure they don’t influence public opinion in unintended ways.

Overall, while China’s strategy is undeniably rooted in a desire for information control, it also has the effect of suppressing misinformation and preventing AI systems from exploiting users. This reactive, hands-on regulation has proven effective in China’s context, even though it might be difficult to apply the same approach in regions like the EU, where values and governance structures differ significantly.


##### **4. Why the EU Can’t Fully Adopt China’s Model**
Although there are lessons to be drawn from China’s AI regulation approach, the EU faces significant challenges that make it nearly impossible to fully adopt the same model. One of the biggest obstacles is the diversity of political and legal systems across the EU. With 27 member states, each with its own unique regulations, cultural considerations, and governance structures, trying to implement a reactive, vertical model like China’s would lead to confusion and inconsistencies across borders. China’s centralized government can swiftly enact regulations with uniform enforcement, but the EU’s political landscape requires a more coordinated and collaborative approach.

The EU’s commitment to democratic values also plays a critical role. In Europe, ensuring the protection of freedom of speech and personal privacy are non-negotiable principles. A model like China’s, which focuses heavily on information control, could easily clash with these core values. The EU AI Act, therefore, takes a broader, more horizontal approach, creating a comprehensive framework that applies to all AI systems while respecting the diverse legal systems and rights of its member states.

That being said, there are aspects of China’s strategy that the EU could adapt rather than adopt. For instance, while the EU cannot fully embrace China’s strict information control measures, it could certainly learn from the way China manages recommendation systems. Ensuring that these systems promote truthful and responsible content is something the EU could pursue more actively. By introducing more transparency and oversight into how AI-driven recommendations work, the EU can combat misinformation without compromising on its core democratic principles.

##### **5. Finding Balance: Regulating AI Recommendation Systems**
As I’ve mentioned in [my previous post](https://nicofirst1.github.io/news/ai_risks/), **AI-driven recommendation systems** can push users toward **isolating** or **divisive** content, exacerbating issues like **loneliness** and **misinformation**. Social media platforms rely heavily on these systems, which often [create echo chambers](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0203958) and amplify [societal disconnection](https://www.psychologytoday.com/us/blog/urban-survival/202404/ai-recommendation-algorithms-can-worsen-loneliness). The loneliness epidemic, in particular, has worsened as these algorithms, designed to maximize engagement, lead users away from meaningful interactions.

China’s [algorithm registry](https://carnegieendowment.org/posts/2022/12/what-chinas-algorithm-registry-reveals-about-ai-governance?lang=en) is one of the practical measures that the EU could adapt. This system requires platforms to provide detailed reports on how their [recommendation algorithms work](https://www.china-briefing.com/news/china-passes-sweeping-recommendation-algorithm-regulations-effect-march-1-2022/), including the datasets they use and their potential impact on public opinion. Implementing a similar system in the EU would increase transparency, allowing regulators and the public to understand how these algorithms prioritize content. This would help flag harmful content and curb the spread of misinformation, while ensuring that companies operate more transparently.

Holding companies accountable for the societal impact of their AI systems is another critical aspect of China’s model that the EU could adopt. In the EU, platforms should be held responsible for the effects of their recommendation algorithms, especially if they contribute to social isolation or spread harmful content. Companies should be required to address and mitigate these outcomes, incentivizing them to design AI systems that promote social well-being instead of solely focusing on user engagement.

Additionally, China’s regulation of automatic pricing systems, aimed at ensuring fairness and preventing exploitation, could inspire the EU’s approach to recommendation systems. Platforms could be encouraged to promote content that fosters social connections and addresses societal issues like mental health. By incentivizing algorithms that benefit users and society at large, the EU can reduce the negative effects of these systems.

That said, the EU must ensure that any regulations it adopts maintain a balance between transparency and the protection of free speech. Unlike China’s focus on information control, the EU should emphasize accountability without infringing on users’ rights. The aim is not to limit what people can see, but to ensure that the systems recommending content do so responsibly and openly.

By adapting China’s practical regulatory elements—such as transparency measures and corporate accountability—the EU can create a framework that fosters responsible AI while upholding democratic principles.

##### **6. Conclusion: Innovation and Integrity**
In summary, while the EU cannot fully adopt China’s strict regulatory model, there are valuable lessons to be learned, particularly when it comes to the regulation of recommendation systems. China’s approach shows that it’s possible to create accountability and transparency in AI systems, ensuring that they promote content that benefits society. However, the EU must achieve this without compromising its core commitment to free expression and innovation.

At the end of the day, we all want AI systems that promote truthful and helpful information—especially in today’s complex world. By taking inspiration from China’s methods while staying true to its own democratic principles, the EU has the chance to lead the way in AI governance. This approach could strike the right balance between fostering innovation and maintaining integrity, ensuring that AI serves the best interests of both individuals and society at large.